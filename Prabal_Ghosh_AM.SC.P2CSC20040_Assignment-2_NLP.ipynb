{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prabal Ghosh Roll-AM.SC.P2CSC20040 Assignment-2 (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Prabal\n",
      "[nltk_data]     Ghosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find edit distance between the strings str1 = ”execution” str2 = ”intention”.(use paper and pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def editDistance(str1, str2, m, n): \n",
    "\n",
    "\tif m == 0: \n",
    "\t\treturn n \n",
    "\tif n == 0: \n",
    "\t\treturn m \n",
    "\tif str1[m-1]== str2[n-1]: \n",
    "\t\treturn editDistance(str1, str2, m-1, n-1) \n",
    "\n",
    "\t\n",
    "\treturn 1 + min(editDistance(str1, str2, m, n-1), # Insert \n",
    "\t\t\t\teditDistance(str1, str2, m-1, n), # Remove \n",
    "\t\t\t\teditDistance(str1, str2, m-1, n-1) # Replace \n",
    "\t\t\t\t) \n",
    "\n",
    "# Driver program to test the above function \n",
    "str1 = \"execution\"\n",
    "str2 = \"intention\"\n",
    "print (editDistance(str1, str2, len(str1), len(str2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Identify any 10 most frequently used token generated in Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"C:/Users/Prabal Ghosh/Desktop/chatbot.txt\")\n",
    "text=file.read()\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text after stop word removal: \n",
      "\n",
      "\n",
      "AI chatbots improve functionality become smarter time progresses. They learn new features adapt required. Intelligent chatbots become intelligent time using NLP machine learning algorithms.\n",
      "Human feedback essential growth advancement AI chatbot. Developers review feedback make relevant changes improve functionality chatbot.Intelligent chatbots game changer organizations looking intelligently interact customers automated manner.\n",
      "The process would genuinely tedious cumbersome create rule-based chatbot level understanding intuition advanced AI chatbot. Understanding goals user extremely important when designing chatbot conversation.\n",
      "The chatbot provided large amount data algorithms process find model give correct answers.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_list = list(text.split(\" \"))\n",
    "removed_text=[w for w in text_list if w not in stop_words]\n",
    "final_result_text=' '.join(removed_text)\n",
    "print(\"final text after stop word removal: \\n\\n\")\n",
    "print(final_result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "words = nltk.tokenize.word_tokenize(final_result_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non stop words are:\n",
      "\n",
      " \n",
      "['AI', 'chatbots', 'improve', 'functionality', 'become', 'smarter', 'time', 'progresses', 'learn', 'new', 'features', 'adapt', 'required', 'Intelligent', 'chatbots', 'become', 'intelligent', 'time', 'using', 'NLP', 'machine', 'learning', 'algorithms', 'Human', 'feedback', 'essential', 'growth', 'advancement', 'AI', 'chatbot', 'Developers', 'review', 'feedback', 'make', 'relevant', 'changes', 'improve', 'functionality', 'chatbot.Intelligent', 'chatbots', 'game', 'changer', 'organizations', 'looking', 'intelligently', 'interact', 'customers', 'automated', 'manner', 'process', 'would', 'genuinely', 'tedious', 'cumbersome', 'create', 'rule-based', 'chatbot', 'level', 'understanding', 'intuition', 'advanced', 'AI', 'chatbot', 'Understanding', 'goals', 'user', 'extremely', 'important', 'designing', 'chatbot', 'conversation', 'chatbot', 'provided', 'large', 'amount', 'data', 'algorithms', 'process', 'find', 'model', 'give', 'correct', 'answers']\n"
     ]
    }
   ],
   "source": [
    "non_stop_words_list=[]\n",
    "Count=0;\n",
    "for i in words:\n",
    "    if i.lower() not in stop_words:\n",
    "        non_stop_words_list.append(i)\n",
    "print(\"Non stop words are:\\n\\n \")\n",
    "print(non_stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 68 samples and 83 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist(non_stop_words_list)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chatbot', 5), ('AI', 3), ('chatbots', 3), ('improve', 2), ('functionality', 2), ('become', 2), ('time', 2), ('algorithms', 2), ('feedback', 2), ('process', 2)]\n"
     ]
    }
   ],
   "source": [
    "frequent_words=fdist.most_common(10)\n",
    "print(frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute Term Frequency of those words.(Ref Online materials for Term Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'chatbot': 1, 'AI': 1, 'chatbots': 1, 'improve': 1, 'functionality': 1, 'become': 1, 'time': 1, 'algorithms': 1, 'feedback': 1, 'process': 1})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostoccur_word = []\n",
    "for word in frequent_words:\n",
    "    mostoccur_word.append(word[0])\n",
    "FreqDist(mostoccur_word)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chatbot', 'AI', 'chatbots', 'improve', 'functionality', 'become', 'time', 'algorithms', 'feedback', 'process']\n"
     ]
    }
   ],
   "source": [
    "frequent_words_list=[]\n",
    "i=0\n",
    "while(i<10):\n",
    "    w=frequent_words[i][0]\n",
    "    frequent_words_list.append(w)\n",
    "    i=i+1\n",
    "print(frequent_words_list)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (3, 6)\t1.0\n",
      "  (4, 5)\t1.0\n",
      "  (6, 8)\t1.0\n",
      "  (7, 1)\t1.0\n",
      "  (8, 4)\t1.0\n",
      "  (9, 7)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idfvectorizer = TfidfVectorizer()\n",
    "tf_idfvectorizer = TfidfVectorizer(stop_words= 'english') \n",
    "X = tf_idfvectorizer.fit_transform(frequent_words_list)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Identify the stems and lemma for those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot  -->  chatbot\n",
      "AI  -->  AI\n",
      "chatbots  -->  chatbot\n",
      "improve  -->  improv\n",
      "functionality  -->  function\n",
      "become  -->  becom\n",
      "time  -->  time\n",
      "algorithms  -->  algorithm\n",
      "feedback  -->  feedback\n",
      "process  -->  process\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "obj = PorterStemmer()\n",
    "for word in mostoccur_word:\n",
    "    print(word, \" --> \", obj.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot --> chatbot\n",
      "AI --> AI\n",
      "chatbots --> chatbots\n",
      "improve --> improve\n",
      "functionality --> functionality\n",
      "become --> become\n",
      "time --> time\n",
      "algorithms --> algorithm\n",
      "feedback --> feedback\n",
      "process --> process\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "check_word = []\n",
    "for word in mostoccur_word:\n",
    "    check_word.append(lemmatizer.lemmatize(word))\n",
    "    print(word,\"-->\",lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Check for any words that are not lemmatised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words that are not lemmatised:\n",
      "\n",
      "chatbot\n",
      "AI\n",
      "chatbots\n",
      "improve\n",
      "functionality\n",
      "become\n",
      "time\n",
      "feedback\n",
      "process\n"
     ]
    }
   ],
   "source": [
    "print(\"words that are not lemmatised:\\n\")\n",
    "for i in range(0,len(frequent_words_list)):\n",
    "    if(frequent_words_list[i]==check_word[i]):\n",
    "        print(check_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words that are  lemmatised:\n",
      "\n",
      "algorithm\n"
     ]
    }
   ],
   "source": [
    "# lemmatised words\n",
    "print(\"words that are  lemmatised:\\n\")\n",
    "for i in range(0,len(frequent_words_list)):\n",
    "    if(frequent_words_list[i] !=check_word[i]):\n",
    "        print(check_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Explain the use of inltk.(Any 5 Apis )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenize API"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "iNLTK - NLTK for Indic languages\n",
    "tokenize(text ,'<code-of-language>')\n",
    "The tokenize method can be used to tokenize the text based on the language inputted at the second argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Word Embeddings API"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from inltk.inltk import get_embedding_vectors\n",
    "vectors = get_embedding_vectors(text, '<code-of-language>')\n",
    "The vectorised word embeddings of inputted text can be obtained through passing the language code, returning an array of vectors\n",
    "representing the word in a specific dimension in the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Identify language API\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from inltk.inltk import identify_language\n",
    "identify_language(text)\n",
    "The API returns the language of the text being passed over as the argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sentence Similarity API"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from inltk.inltk import get_sentence_similarity\n",
    "get_sentence_similarity(sentence1, sentence2, '<code-of-language>', cmp = cos_sim)\n",
    "The API returns the similarity or distance measure of the two sentences by passing over the language code, the similarity measure\n",
    "can be cosine similarity etc, which in this case has been used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Sentence Encoding API"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from inltk.inltk import get_sentence_encoding\n",
    "get_sentence_encoding(text, '<code-of-language>')\n",
    "The API returns a 400 dimensional encoding of the sentence from ULMFiT LM Encoder of the respective language code passed over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
